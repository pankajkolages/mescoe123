{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93440bc2-89a8-4d2f-97cb-0b04eb0aed36",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text Analytics\n",
    "1. Extract Sample document and apply following document preprocessing methods:\n",
    "Tokenization, POS Tagging, stop words removal, Stemming and Lemmatization.\n",
    "2. Create representation of document by calculating Term Frequency and Inverse Document\n",
    "Frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4366e6a-5e9f-4d14-81fb-f46c5ee995e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_doc = \"\"\"\n",
    "Natural Language Processing (NLP) is a fascinating field of Artificial Intelligence (AI) that focuses on the interaction between computers and human languages. \n",
    "The goal is to enable computers to understand, interpret, and generate human language in a valuable way.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdc80a04-612f-4bf0-b6e5-40933c9f60c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: joblib in c:\\users\\jyoti\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl.metadata (41 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\jyoti\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   -------------------- ------------------- 0.8/1.5 MB 6.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 6.7 MB/s eta 0:00:00\n",
      "Downloading regex-2024.11.6-cp313-cp313-win_amd64.whl (273 kB)\n",
      "Using cached click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, click, nltk\n",
      "Successfully installed click-8.1.8 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0250d19a-0f68-439b-994d-1705a266058d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jyoti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\jyoti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\jyoti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\jyoti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jyoti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\jyoti\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62a35896-4221-4858-aca5-3d06babe4d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Tokens:\n",
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'fascinating', 'field', 'of', 'Artificial', 'Intelligence', '(', 'AI', ')', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'human', 'languages', '.', 'The', 'goal', 'is', 'to', 'enable', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', 'in', 'a', 'valuable', 'way', '.']\n"
     ]
    }
   ],
   "source": [
    "#Tokenisation\n",
    "tokens = word_tokenize(sample_doc)\n",
    "print(\"\\nTokens:\")\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "16a21a96-c287-485e-a7d2-bc718f0b9d19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "POS Tags:\n",
      "[('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('(', '('), ('NLP', 'NNP'), (')', ')'), ('is', 'VBZ'), ('a', 'DT'), ('fascinating', 'JJ'), ('field', 'NN'), ('of', 'IN'), ('Artificial', 'JJ'), ('Intelligence', 'NNP'), ('(', '('), ('AI', 'NNP'), (')', ')'), ('that', 'WDT'), ('focuses', 'VBZ'), ('on', 'IN'), ('the', 'DT'), ('interaction', 'NN'), ('between', 'IN'), ('computers', 'NNS'), ('and', 'CC'), ('human', 'JJ'), ('languages', 'NNS'), ('.', '.'), ('The', 'DT'), ('goal', 'NN'), ('is', 'VBZ'), ('to', 'TO'), ('enable', 'JJ'), ('computers', 'NNS'), ('to', 'TO'), ('understand', 'VB'), (',', ','), ('interpret', 'VB'), (',', ','), ('and', 'CC'), ('generate', 'VB'), ('human', 'JJ'), ('language', 'NN'), ('in', 'IN'), ('a', 'DT'), ('valuable', 'JJ'), ('way', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "#POS Tagging\n",
    "pos_tags = pos_tag(tokens)\n",
    "print(\"\\nPOS Tags:\")\n",
    "print(pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3af9af26-0c3a-4443-b5d2-caa3626b967d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Stop Words Removal:\n",
      "{'m', 'mustn', 'yourself', \"she'd\", 'shan', 'an', 'too', 'will', 'my', 'not', 'own', 'being', 'don', 'by', \"i'd\", 'after', \"aren't\", 'just', 'so', 'how', 'ours', 'while', 'our', \"you've\", \"needn't\", 't', 'now', 'he', \"i'll\", \"she's\", 'with', 'nor', 'which', 'those', 'y', 'other', 'a', 'does', \"isn't\", 'same', 'once', 'did', 'that', 'do', 'such', 'but', 'until', 'has', 'we', \"it'll\", 'am', \"hasn't\", 'couldn', 'its', 'because', \"mustn't\", 'at', 'himself', 'off', 'they', \"they're\", 'isn', 'ain', 'themselves', \"she'll\", \"you're\", 'of', 'few', 'then', \"they've\", 'there', \"won't\", 'in', 'be', 'him', 'where', \"shouldn't\", 'won', 'you', 'doesn', 'been', 'her', 'me', \"shan't\", \"it'd\", 'about', \"doesn't\", 'it', \"he'll\", 'into', 'more', 'down', 'both', 'didn', 'as', 'here', 'their', \"wouldn't\", 'weren', 'all', \"i've\", 'between', 'needn', 'on', 'myself', 'haven', 'wouldn', \"hadn't\", 'each', 'were', 'very', 'out', 'most', 'd', 'she', 'was', \"didn't\", 'before', 'what', \"couldn't\", 'hadn', 'is', 'i', \"haven't\", \"we're\", 's', 'or', 'ma', \"we've\", 'doing', 'if', 'theirs', 'itself', \"i'm\", 'wasn', 're', 'who', \"mightn't\", 'below', 'any', 'having', 'are', \"don't\", 'should', 'll', \"you'd\", 'yourselves', 'this', \"that'll\", 'mightn', \"it's\", 've', \"weren't\", 'to', \"you'll\", 'only', 'can', 'when', 'over', 'whom', 'some', 'for', 'above', 'shouldn', 'your', 'during', 'why', \"should've\", 'and', 'from', 'against', \"he's\", 'further', 'the', 'through', 'aren', 'up', 'no', \"they'd\", 'yours', 'these', \"they'll\", 'had', \"we'll\", 'herself', 'again', \"wasn't\", 'than', 'them', \"we'd\", 'hasn', 'o', 'ourselves', 'hers', \"he'd\", 'have', 'under', 'his'}\n"
     ]
    }
   ],
   "source": [
    "# Stop Words Removal\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words and word.isalpha()]\n",
    "print(\"\\nAfter Stop Words Removal:\")\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7233eb32-84b6-4c8d-92a3-b0f5090a41f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AFter Stemming\n",
      "['natur', 'languag', 'process', 'nlp', 'fascin', 'field', 'artifici', 'intellig', 'ai', 'focus', 'interact', 'comput', 'human', 'languag', 'goal', 'enabl', 'comput', 'understand', 'interpret', 'gener', 'human', 'languag', 'valuabl', 'way']\n"
     ]
    }
   ],
   "source": [
    "#Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "print(\"\\nAFter Stemming\")\n",
    "print(stemmed_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2591ffae-2de0-4657-ab61-305b7db13b7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After Lemmatization\n",
      "['Natural', 'Language', 'Processing', 'NLP', 'fascinating', 'field', 'Artificial', 'Intelligence', 'AI', 'focus', 'interaction', 'computer', 'human', 'language', 'goal', 'enable', 'computer', 'understand', 'interpret', 'generate', 'human', 'language', 'valuable', 'way']\n"
     ]
    }
   ],
   "source": [
    "#Lemmatization\n",
    "lemmatizer= WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "print(\"\\nAfter Lemmatization\")\n",
    "print(lemmatized_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "51bd475f-985d-40ae-b991-5d79f347a471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Matrix:\n",
      "[[0.14002801 0.28005602 0.14002801 0.14002801 0.28005602 0.14002801\n",
      "  0.14002801 0.14002801 0.14002801 0.14002801 0.14002801 0.28005602\n",
      "  0.14002801 0.14002801 0.14002801 0.14002801 0.28005602 0.28005602\n",
      "  0.14002801 0.14002801 0.14002801 0.14002801 0.14002801 0.14002801\n",
      "  0.14002801 0.28005602 0.28005602 0.14002801 0.14002801 0.14002801]]\n",
      "\n",
      "Features Names (Words):\n",
      "['ai' 'and' 'artificial' 'between' 'computers' 'enable' 'fascinating'\n",
      " 'field' 'focuses' 'generate' 'goal' 'human' 'in' 'intelligence'\n",
      " 'interaction' 'interpret' 'is' 'language' 'languages' 'natural' 'nlp'\n",
      " 'of' 'on' 'processing' 'that' 'the' 'to' 'understand' 'valuable' 'way']\n"
     ]
    }
   ],
   "source": [
    "#Docment Representation TF and IDF\n",
    "\n",
    "vectorizer= TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform([sample_doc])\n",
    "\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(tfidf_matrix.toarray())\n",
    "\n",
    "print(\"\\nFeatures Names (Words):\")\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92540e07-079e-4a39-96d4-1ca62529edd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
